{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters and other constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "MEMORY_SIZE = 100000\n",
    "ACTOR_LEARNING_RATE = 0.0001\n",
    "CRITIC_LEARNING_RATE = 0.001\n",
    "ACTOR_LAYER_SIZES = [256, 128]\n",
    "CRITIC_LAYER_SIZES = [256, 128]\n",
    "BATCH_SIZE = 256\n",
    "NUMBER_OF_EPISODES = 300\n",
    "K_STEPS = 1\n",
    "TAU = 0.001\n",
    "TRAIN_STEPS = 1\n",
    "NOISE = 0.1\n",
    "NOISE_DECAY = 0.99\n",
    "GOAL_SCORE = 30\n",
    "ENV_PATH = 'Reacher_Windows_x86_64/Reacher.exe'\n",
    "ACTOR_PATH = 'actor.pth'\n",
    "CRITIC_PATH = 'critic.pth'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Auxiliar functions\n",
    "\n",
    "Auxiliar functions used in the main program"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# If GPU is available, use it for trainining the agent's DQN\n",
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print('CUDA is available. Training on GPU.')\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        print('CUDA is not available. Training on CPU.')\n",
    "        return torch.device('cpu')\n",
    "\n",
    "# Create Unity environment\n",
    "def create_environment(train_mode, path_to_env):\n",
    "    env = UnityEnvironment(file_name=path_to_env)\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "    state_size = len(env_info.vector_observations[0])\n",
    "    action_size = brain.vector_action_space_size\n",
    "    return env, env_info, brain_name, state_size, action_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class ReplayMemory\n",
    "\n",
    "Memory of transitions for experience replay."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    # Create replay memory\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.states = np.zeros((MEMORY_SIZE, state_size))\n",
    "        self.states_next = np.zeros((MEMORY_SIZE, state_size))\n",
    "        self.actions = np.zeros((MEMORY_SIZE, action_size))\n",
    "        self.rewards = np.zeros(MEMORY_SIZE)\n",
    "        self.terminal_states = np.zeros(MEMORY_SIZE)\n",
    "        self.current_size = 0\n",
    "\n",
    "    # Store a transition (s,a,r,s') in the replay memory\n",
    "    def store_transition(self, state, action, reward, state_next, terminal_state):\n",
    "        i = self.current_size\n",
    "        self.states[i] = state\n",
    "        self.states_next[i] = state_next\n",
    "        self.actions[i] = action\n",
    "        self.rewards[i] = reward\n",
    "        self.terminal_states[i] = terminal_state\n",
    "        self.current_size = i + 1\n",
    "\n",
    "        if self.current_size >= MEMORY_SIZE - 1:\n",
    "            self.current_size = 0\n",
    "\n",
    "    # Generate a random sample of transitions from the replay memory\n",
    "    def sample_memory(self, batch_size):\n",
    "        batch = np.random.choice(self.current_size, batch_size)\n",
    "        states = torch.from_numpy(self.states[batch]).float().to(device)\n",
    "        states_next = torch.from_numpy(self.states_next[batch]).float().to(device)\n",
    "        rewards = torch.from_numpy(self.rewards[batch]).float().to(device)\n",
    "        actions = torch.from_numpy(self.actions[batch]).long().to(device)\n",
    "        terminal_states = torch.from_numpy(self.terminal_states[batch]).float().to(device)\n",
    "        return states, actions, rewards, states_next, terminal_states"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class OUNoise\n",
    "\n",
    "Ornstein-uhlenbeck noise to add some exploration to the agent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.size = size  # num agents added\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = None\n",
    "        random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        # dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classes Actor & Critic\n",
    "\n",
    "Actor and critic models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    # Create neural network\n",
    "    def __init__(self, state_size, action_size, layer_sizes):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_size, layer_sizes[0])\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(layer_sizes[0])\n",
    "        self.layer_2 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(layer_sizes[1])\n",
    "        self.layer_3 = nn.Linear(layer_sizes[1], action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.layer_1.weight.data.uniform_(*hidden_init(self.layer_1))\n",
    "        self.layer_2.weight.data.uniform_(*hidden_init(self.layer_2))\n",
    "        self.layer_3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, state):\n",
    "        x = torch.nn.functional.leaky_relu(self.batch_norm_1(self.layer_1(state)))\n",
    "        x = torch.nn.functional.leaky_relu(self.batch_norm_2(self.layer_2(x)))\n",
    "        x = torch.tanh(self.layer_3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    # Create neural network\n",
    "    def __init__(self, state_size, action_size, layer_sizes):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_size, layer_sizes[0])\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(layer_sizes[0])\n",
    "        self.layer_2 = nn.Linear(layer_sizes[0]+action_size, layer_sizes[1])\n",
    "        self.layer_3 = nn.Linear(layer_sizes[1], 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # Weight initialization\n",
    "    def reset_parameters(self):\n",
    "        self.layer_1.weight.data.uniform_(*hidden_init(self.layer_1))\n",
    "        self.layer_2.weight.data.uniform_(*hidden_init(self.layer_2))\n",
    "        self.layer_3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, state, action):\n",
    "        x_s = torch.nn.functional.leaky_relu(self.batch_norm_1(self.layer_1(state)))\n",
    "        x = torch.cat((x_s, action), dim=1)\n",
    "        x = torch.nn.functional.leaky_relu((self.layer_2(x)))\n",
    "        x = self.layer_3(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class Agent\n",
    "\n",
    "DDPG agent with actor and critic networks, as well as their respective target networks.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    # Create DDPG agent\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.scores = []\n",
    "        self.memory = ReplayMemory(state_size, action_size)\n",
    "        self.noise = OUNoise((1, action_size), 0)\n",
    "        self.actor_local = Actor(state_size, action_size, ACTOR_LAYER_SIZES).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, ACTOR_LAYER_SIZES).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(), lr=ACTOR_LEARNING_RATE)\n",
    "        self.soft_update(self.actor_local, self.actor_target, 0)\n",
    "        self.critic_local = Critic(state_size, action_size, CRITIC_LAYER_SIZES).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, CRITIC_LAYER_SIZES).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(), lr=CRITIC_LEARNING_RATE)\n",
    "        self.soft_update(self.critic_local, self.critic_target, 0)\n",
    "\n",
    "    # Store a tuple (s, a, r, s') for experience replay\n",
    "    def remember(self, state, action, reward, next_state, terminal_state):\n",
    "        self.memory.store_transition(state, action, reward, next_state, terminal_state)\n",
    "\n",
    "    # Select action following the current policy\n",
    "    def act(self, state, add_noise=True):\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(torch.from_numpy(state).float().to(device)).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    # Perform an action over the environment\n",
    "    def step(self, action, brain_name, env):\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        reward = env_info.rewards[0]\n",
    "        next_state = env_info.vector_observations\n",
    "        terminal_state = env_info.local_done[0]\n",
    "        return reward, next_state, terminal_state\n",
    "\n",
    "    # Update policy and value function.\n",
    "    def learn(self, total_steps):\n",
    "        if self.memory.current_size < BATCH_SIZE: return\n",
    "\n",
    "        # Sample a batch of experiences from memory and add some noise to actions\n",
    "        states, actions, rewards, next_states, terminal_states = self.memory.sample_memory(BATCH_SIZE)\n",
    "\n",
    "        # ------------------ CRITIC TRAINING ------------------\n",
    "        # Predict next actions and their Q values using target networks\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current state\n",
    "        q_targets = rewards.unsqueeze(1) + (GAMMA * q_targets_next * (1 - terminal_states).unsqueeze(1))\n",
    "        # Compute actual q value for current states and actions\n",
    "        q_expected = self.critic_local(states, actions)\n",
    "        # Compute critic loss and minimize it\n",
    "        critic_loss = nn.functional.mse_loss(q_expected, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ------------------ ACTOR TRAINING ------------------\n",
    "        # Compute actor loss\n",
    "        actions_predicted = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_predicted).mean()\n",
    "        # Minimize it\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "\n",
    "        # ------------------ SOFT UPDATING ------------------\n",
    "        if total_steps % K_STEPS == 0:\n",
    "            self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "            self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "\n",
    "    # Add noise to actions to encourage exploration and decrease it slightly\n",
    "    def add_noise(self, action):\n",
    "        action = action + (self.noise * np.random.randn(self.action_size))\n",
    "        self.noise = self.noise * self.noise_decay\n",
    "        return action\n",
    "\n",
    "    # Soft update a target network\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    # Add the obtained score in a list to be presented later\n",
    "    def add_score(self, score):\n",
    "        self.scores.append(score)\n",
    "\n",
    "    # Display the obtained scores graphically\n",
    "    def display_scores_graphically(self):\n",
    "        plt.plot(self.scores)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main program"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Training on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   1:   average score 0.5099999886006117   transitions: 1001)  score:  0.5099999886006117 \r\n",
      "Episode   2:   average score 0.3299999926239252   transitions: 2002)  score:  0.14999999664723873 \r\n",
      "Episode   3:   average score 0.3433333256592353   transitions: 3003)  score:  0.36999999172985554 \r\n",
      "Episode   4:   average score 0.4349999902769923   transitions: 4004)  score:  0.7099999841302633 \r\n",
      "Episode   5:   average score 0.7379999835044145   transitions: 5005)  score:  1.9499999564141035 \r\n",
      "Episode   6:   average score 0.9366666457305352   transitions: 6006)  score:  1.9299999568611383 \r\n",
      "Episode   7:   average score 0.8242856958614928   transitions: 7007)  score:  0.14999999664723873 \r\n",
      "Episode   8:   average score 0.9137499795760959   transitions: 8008)  score:  1.5399999655783176 \r\n",
      "Episode   9:   average score 0.9211110905226734   transitions: 9009)  score:  0.979999978095293 \r\n",
      "Episode  10:   average score 0.8929999800398946   transitions: 10010)  score:  0.6399999856948853 \r\n",
      "Episode  11:   average score 0.8372727085582234   transitions: 11011)  score:  0.2799999937415123 \r\n"
     ]
    }
   ],
   "source": [
    "# Checking if GPU is available\n",
    "device = get_default_device()\n",
    "\n",
    "env, env_info, brain_name, state_size, action_size = create_environment(True, ENV_PATH)\n",
    "agent = Agent(state_size, action_size)\n",
    "scores = []\n",
    "goal_reached = False\n",
    "episode = 0\n",
    "total_steps = 1\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "while (episode < NUMBER_OF_EPISODES) and not goal_reached:\n",
    "    episode += 1\n",
    "    step = 1\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "    end_episode = False\n",
    "    reward_accumulated = 0\n",
    "    while not end_episode:\n",
    "        # Perform an action for the current state, execute the action, retrieve env info and store transition into memory\n",
    "        action = agent.act(state)\n",
    "        reward, next_state, terminal_state = agent.step(action, brain_name, env)\n",
    "        agent.remember(state, action, reward, next_state, terminal_state)\n",
    "\n",
    "        # Learn using a batch of experience stored in memory\n",
    "        if total_steps % TRAIN_STEPS == 0:\n",
    "            agent.learn(step)\n",
    "\n",
    "        # Detect end of episode\n",
    "        if terminal_state:\n",
    "            reward_accumulated += reward\n",
    "            agent.add_score(reward_accumulated)\n",
    "            scores.append(reward_accumulated)\n",
    "            if episode < 100:\n",
    "                avg_score = sum(scores) / len(scores)\n",
    "            else:\n",
    "                avg_score = sum(scores[-100:]) / 100\n",
    "            if avg_score >= GOAL_SCORE:\n",
    "                goal_reached = True\n",
    "            print(\"Episode {0:>3}: \".format(episode),\n",
    "                  \" average score {0:>3} \".format(avg_score),\n",
    "                  \" transitions: \" + str(agent.memory.current_size) + \")\",\n",
    "                  \" score: \", reward_accumulated, '\\r')\n",
    "            end_episode = True\n",
    "        else:\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            total_steps += 1\n",
    "            reward_accumulated += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Printing some info\n",
    "if goal_reached:\n",
    "    print(\"Reached goal sucessfully.\")\n",
    "    torch.save(agent.actor_local.state_dict(), ACTOR_PATH)\n",
    "    torch.save(agent.critic_local.state_dict(), CRITIC_PATH)\n",
    "\n",
    "else:\n",
    "    print(\"Failure to reach the goal.\")\n",
    "\n",
    "print(\"Time:\", round((time.perf_counter() - start_time) / 60), \"minutes\")\n",
    "\n",
    "agent.display_scores_graphically()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
